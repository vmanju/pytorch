{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Intro to DL for NLP: Module 1 Workbook",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/vmanju/pytorch/blob/master/Intro_to_DL_for_NLP_Module_1_Workbook.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "9J7p406abzgl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Introduction to Deep Learning for Natural Language Processing\n",
        "## Notebook 1: The Basics\n",
        "\n",
        "This notebook provides a general introduction to natural language processing (NLP) and deep learning in PyTorch.\n",
        "\n",
        "In order to save changes to the notebook, you will need to save a separate copy that you own. Click **File > Save a copy in Drive...** or choose one of the other saving options that allows you to make a copy of this notebook.\n",
        "\n",
        "Code sections of the notebook appear in grey cells. To run the code in a cell, hover over the brackets in the upper left corner of the cell and click the play button or **Shift+Enter**. You can edit the code in any cell.\n",
        "\n",
        "When you have completed the lab, return to Trailhead to enter your answers to the exercises in the quiz section and get points.\n",
        "\n",
        "Feel free to contact Bryan McCann (bmccann@salesforce.com) with any questions or to report issues."
      ]
    },
    {
      "metadata": {
        "id": "gvh_MLh0FTtj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## PyTorch"
      ]
    },
    {
      "metadata": {
        "id": "fKUS5ND_Gd0z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Installing PyTorch and Lab dependencies\n",
        "\n",
        "Make sure to run this cell in order to get the dependencies for this part of the lab. "
      ]
    },
    {
      "metadata": {
        "id": "iigaj3TPFDCF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from random import shuffle\n",
        "import collections\n",
        "import numpy as np\n",
        "\n",
        "!pip3 install http://download.pytorch.org/whl/cu80/torch-0.4.0-cp36-cp36m-linux_x86_64.whl\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def println(*x):\n",
        "  print(*x)\n",
        "  print()\n",
        "  \n",
        "TODO = \"Not Yet Implemented\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QZnwGZeZNipz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Tensors\n",
        "\n",
        "PyTorch makes widespread use of Tensor objects. There are two ways to create Tensors in PyTorch. The first way is to create a (possibly nested) list of numerical values."
      ]
    },
    {
      "metadata": {
        "id": "FmDGGBkm9Djk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "println(\"1-dimensional tensors can be created from nested lists of depth 1\")\n",
        "one_d_tensor = torch.tensor([1, 2, 3])\n",
        "println(one_d_tensor) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n5Xxpb23o5Uq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "println(\"2-dimensional tensors can be created from nested lists of depth 2\")\n",
        "two_d_tensor = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
        "println(two_d_tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6NCdBWY9o6XZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "println(\"3-dimensional tensors can be created from nested lists of depth 3\")\n",
        "three_d_tensor = torch.tensor([[[1.1, 2.2, 3.3], [4.4, 5.5, 6.6]], [[7.7, 8.8, 9.9], [10.1, 11.1, 12.2]]])\n",
        "println(three_d_tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sVmMkINrp9a9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Indexing"
      ]
    },
    {
      "metadata": {
        "id": "QmmdElIx--cz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For those with some background in linear algebra, 1-dimensional tensors correspond to vectors and 2-dimensional tensors correspond to matrices. A 0-dimensional tensor corresponds to a single scalar value."
      ]
    },
    {
      "metadata": {
        "id": "7Krtd866_SDN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "println(\"0-dimensional tensor created by indexing into a 2-dimensional Tensor.\")\n",
        "two_d_tensor = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
        "scalar = two_d_tensor[0][0]\n",
        "println(scalar)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iBMKkVYco_XF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "println(\"0-dimensional tensor created by indexing into a 3-dimensional Tensor.\")\n",
        "three_d_tensor = torch.tensor([[[1.1, 2.2, 3.3], [4.4, 5.5, 6.6]], [[7.7, 8.8, 9.9], [10.1, 11.1, 12.2]]])\n",
        "scalar = three_d_tensor[0][0][0]\n",
        "println(scalar)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MQRvtwYN_tHl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can extract lower dimensional tensors from higher dimensional ones."
      ]
    },
    {
      "metadata": {
        "id": "VNu8eJ33_fez",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "println(\"1-dimensional tensor created by indexing into a 2-dimensional Tensor.\")\n",
        "vector = two_d_tensor[0]\n",
        "println(vector)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G2RQAlcJpKms",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "println(\"2-dimensional tensor created by indexing into a 3-dimensional Tensor.\")\n",
        "matrix = three_d_tensor[0]\n",
        "println(matrix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8vR9kGIYqBuo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Initializing Tensors"
      ]
    },
    {
      "metadata": {
        "id": "9cMf0mMwAr8J",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Pytorch offers a variety of ways to initialize tensors that include all ones, all zeros, in increasing order, amongst [others](https://pytorch.org/docs/master/torch.html?highlight=randn#creation-ops). Below are examples of randomly initializing a tensor, initializing with a sequence of numerical values, and then taking the same sequence of values but 'viewing' them in a different way. This latter idea of 'viewing' or reshaping the tensor will be sprinkled throughout PyTorch code. "
      ]
    },
    {
      "metadata": {
        "id": "fSUV1wX2BCFX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "println(\"Randomly initialize a 3d tensor of size (1, 2, 3)\")\n",
        "randomly_initialized_tensor = torch.randn(1, 2, 3)\n",
        "println(randomly_initialized_tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DY4hBE4bpXhZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "println(\"Initialize to a sequence of ten numbers\")\n",
        "range_tensor = torch.arange(start=0, end=10, step=1)\n",
        "println(range_tensor)\n",
        "print(range_tensor.size())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Cl74cbEspagP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "println(\"Zero all entries in a tensor\")\n",
        "print(range_tensor.zero_())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Eg_Qcq2OpbSV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "println(\"Fill with a particular value\")\n",
        "print(range_tensor.fill_(1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CpCqx_w_qXJu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Exercise 1"
      ]
    },
    {
      "metadata": {
        "id": "Hfnsl8CDqamb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Set the seed for the random number to 123 generator for consistent, reproducible results\n",
        "torch.manual_seed(123)\n",
        "\n",
        "println(\"Randomly initialize a 3d tensor of size (2,3,4)\")\n",
        "randomly_initialized_tensor = TODO\n",
        "println(randomly_initialized_tensor)\n",
        "\n",
        "println(\"What is the entry of randomly_initialized_tensor indexed by (0, 2, 3)?\")\n",
        "print('Answer to Exercise 1: ', TODO)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ixo94ksPqKz8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Reshaping and Transposing"
      ]
    },
    {
      "metadata": {
        "id": "1VvccfRgpYjz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "println(\"Initialize to a sequence of ten numbers, but then group them into two groups of five\")\n",
        "range_tensor_reshaped = range_tensor.view((2, 5))\n",
        "println(range_tensor_reshaped)\n",
        "print(range_tensor_reshaped.size())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6JANK2ivpZg0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "println(\"Transpose a matrix by switching dimension 0 and dimension 1\")\n",
        "range_tensor_reshaped_transposed = range_tensor_reshaped.transpose(0,1)\n",
        "print(range_tensor_reshaped_transposed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aHpzt9fErHx9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Tensor Operations"
      ]
    },
    {
      "metadata": {
        "id": "4Q0pIwVbkkMs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "PyTorch offers quite a few [operations](https://pytorch.org/docs/master/torch.html?#tensors) for working with Tensors."
      ]
    },
    {
      "metadata": {
        "id": "qeTE5bJiftEo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "A = B = torch.arange(start=0, end=10, step=1).view((2, 5))\n",
        "println('A:', A)\n",
        "print('B:', B)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KrDaQ1brrTZo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "println('Element-wise addition: A + B')\n",
        "print(A + B)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "51lDNkAMrVCU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "println('Element-wise subtraction: A - B')\n",
        "print(A - B)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pYJLM31orWHU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "println('Element-wise multiplication: A * B')\n",
        "print(A * B)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0Gu4VACprXew",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "println('Element-wise division: A / B (notice the nan)')\n",
        "print(A / B)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sSyO6NiyrYhl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "println('Matrix multiplication: A @ B.transpose(0, 1)')\n",
        "print(A @ B.transpose(0, 1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JITgGd5arZhQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "println('Concatenation along dimension 0: torch.cat([A, B], dim=0)')\n",
        "print(torch.cat([A, B], dim=0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hOMHhFN2ragP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "println('Concatenation along dimension 1: torch.cat([A, B], dim=1)')\n",
        "print(torch.cat([A, B], dim=1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4uJSfDA8r2Pa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Exercises 2-3: Comparison Operations"
      ]
    },
    {
      "metadata": {
        "id": "SBqDlkTirEcx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Set the seed for the random number generator for consistent,\n",
        "# reproducible results\n",
        "torch.manual_seed(123)\n",
        "\n",
        "# create a randomly initialized tensor using randn (default settings)\n",
        "# of size (4, 3, 9)\n",
        "x = torch.randn(4, 3, 9)\n",
        "\n",
        "println(\"What is the maximum value of the tensor you created?\") \n",
        "# (https://pytorch.org/docs/stable/torch.html#comparison-ops)\n",
        "print('Answer to Exercise 2: ', TODO)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pXbwxZu6sb6G",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Set the seed for the random number generator for consistent,\n",
        "# reproducible results\n",
        "torch.manual_seed(123)\n",
        "\n",
        "# initialize a tensor to be a sequence of numbers from 0 to 100 \n",
        "# and reshape it to be of size 2, 25, 2\n",
        "x = torch.arange(start=0, end=100).reshape(2, 25, 2)\n",
        "\n",
        "println(\"What is the max over dimension 1?\")\n",
        "# (https://pytorch.org/docs/stable/torch.html#comparison-ops)\n",
        "# note that the max over a dimension will return the indices of the maximum\n",
        "# values as well. We only want the maximum values.\n",
        "print('Answer to Exercise 3: ', TODO) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ochWwnQ5sQdd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Exercsise 4-5: Reduction Operations"
      ]
    },
    {
      "metadata": {
        "id": "4YmV8EulsTA-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Set the seed for the random number generator for consistent, reproducible results\n",
        "torch.manual_seed(123)\n",
        "\n",
        "# create a randomly initialized tensor using randn (default settings)\n",
        "# of size (5, 8, 7)\n",
        "x = torch.randn(5, 8, 7)\n",
        "\n",
        "println(\"What is the sum of this same tensor?\")\n",
        "# (https://pytorch.org/docs/stable/torch.html#reduction-ops)\n",
        "print('Answer to Exercise 4: ', TODO)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zK96QqpuuxW2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Set the seed for the random number generator for consistent,\n",
        "# reproducible results\n",
        "torch.manual_seed(123)\n",
        "\n",
        "# initialize a tensor to be a sequence of numbers from 0 to 100 \n",
        "# and reshape it to be of size 2, 25, 2\n",
        "x = torch.arange(start=0, end=100).reshape(2, 25, 2)\n",
        "\n",
        "println(\"What is the sum over dimension 1?\")\n",
        "# (https://pytorch.org/docs/stable/torch.html#comparison-ops)\n",
        "# note that the max over a dimension will return the indices of the maximum\n",
        "# values as well. We only want the maximum values.\n",
        "print('Answer to Exercise 5: ', TODO) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fQxhz2Rc8-Bo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### torch.nn.functional\n",
        "[torch.nn.functional](https://pytorch.org/docs/stable/nn.html#torch-nn-functional) contains a variety of functions that are often used in deep learning.\n"
      ]
    },
    {
      "metadata": {
        "id": "eM79Js1w9JE-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "println(\"Create a 2d tensor of size (2,2) with 1s along the diagonal and -1s elsewhere.\")\n",
        "A = torch.empty(2,3).fill_(-1) + 2 * torch.eye(2,3)\n",
        "println(A)\n",
        "\n",
        "println(\"tanh(A) maps entries to be between -1 and 1\")\n",
        "println(F.tanh(A))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KRlPOvc2wi4T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "println(\"sigmoid(A)maps entries to be between 0 and 1\")\n",
        "println(F.sigmoid(A))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M9kmj6Hiwpr0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "println(\"relu(A) sets negative entries to 0\")\n",
        "println(F.relu(A))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NDyEpuiowqWQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "println(\"softmax(A, dim=1) ensures that the rows of A (dimension 1) sum to 1\")\n",
        "println(F.softmax(A, dim=1)) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FgpCJ3ODvF0f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# For input x, matrix A, and vector b, we can compute linear transformations\n",
        "A = torch.arange(start=1, end=7).reshape(2, 3)\n",
        "b = torch.tensor([1., 2])\n",
        "x = torch.tensor([1., 1, 1])\n",
        "println('A:', A)\n",
        "println('b:', b)\n",
        "println('x:', x)\n",
        "\n",
        "println('Ax\\' + b: ', A.matmul(x) + b)\n",
        "y = F.linear(input=x, weight=A, bias=b)\n",
        "print('F.linear(x, A, b): ', y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3t6Jf0svvGvn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# We can create more complex functions by applying series of linear and non-linear functions\n",
        "# In this case, we'll take input vectors, \n",
        "# apply a random linear transformation that reduces the output to a single number,\n",
        "# and then we will map that number to be between 0 and 1 using the sigmoid function\n",
        "def more_complex_function(x):\n",
        "    assert x.dim() == 1\n",
        "    A = torch.randn(1, x.size(0))\n",
        "    b = torch.randn(1)\n",
        "    return F.sigmoid(F.linear(input=x, weight=A, bias=b))\n",
        "  \n",
        "x = torch.tensor([2.,2, 2])\n",
        "y = more_complex_function(x)\n",
        "println('x: ', x)\n",
        "println('y: ', y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6d-roEouNU5U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Deep Learning with PyTorch"
      ]
    },
    {
      "metadata": {
        "id": "2ok0TvVBRM64",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In `more_complex_function` above, `A` and `b` are called **parameters**, which is to say that they are values that are set (in this case randomly by us) that affect the final output. `x` is the input, and `y` is the output. \n",
        "\n",
        "`more_complex_function` defines how to get the output `y` from the input `x`. In general, we will be concerned with finding  `more_complex_function`s that capture a meaningful relationship between `x` and `y`. Most often 'meaningful' will itself mean that `more_complex_function` can take input `x` and predict the value of `y`.\n",
        "\n",
        "In this case, `more_complex_function` is not likely to be very useful since the parameters are randomly set each time the function is used. If we did have a certain kind of desired behavior for `more_complex_function`, we could try setting all the parameters by hand. We could add more linear and non-linear transformations and continue to add more parameters that we tweak until the right behavior is found. This would get more difficult with more complex functions, and it is not a sustainable way to find the meaningful relationships we are after. In machine learning, and so in deep learning as well, we will attempt to learn the parameters for these complex functions instead of setting them randomly or by hand."
      ]
    },
    {
      "metadata": {
        "id": "2e-wdGNoFR8l",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### torch.nn\n",
        "[torch.nn](https://pytorch.org/docs/stable/nn.html#torch-nn) contains the most frequently used building blocks, or layers, for deep learning architectures. These layers, or [Modules](https://pytorch.org/docs/stable/nn.html#module) in PyTorch, are wrappers around parameters grouped with linear and non-linear transformations arranged in ways that have proven useful. We can then use these layers to create our own more complex Modules, which are typically neural networks."
      ]
    },
    {
      "metadata": {
        "id": "9jmU8AKrWX9G",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class MoreComplexModule(nn.Module): \n",
        "  # sub-class nn.Module so that we can register and eventually learn parameters\n",
        "  \n",
        "  def __init__(self, input_size, output_size):\n",
        "      # We have to initialize the module before setting self.linear below \n",
        "      # so that it has everything necessary to register the parameters \n",
        "      # associated with the new nn.Linear\n",
        "      super().__init__()\n",
        "      \n",
        "      # nn.Linear randomly initializes a set of parameters A \n",
        "      # and b (if bias=True) just like we did in more_complex_function above.\n",
        "      # The difference here is that the nn.Linear module also registers \n",
        "      # A and b as nn.Parameters so that they can be learned\n",
        "      self.linear = nn.Linear(input_size, output_size)\n",
        "      \n",
        "  # forward defines the behavior of an \n",
        "  # instantiated MoreComplexModule when called\n",
        "  # This is essentially the same kind of behavior we defined\n",
        "  # for more_complex_function above, except that A and b can now be learned\n",
        "  # and are already assigned to self.linear, so we don't need to pass them in\n",
        "  def forward(self, x):\n",
        "      return F.sigmoid(self.linear(x))\n",
        "  \n",
        "x = torch.tensor([2.,2, 2])\n",
        "mcm = MoreComplexModule(x.size(0), 1) # MoreComplexModule.__init__\n",
        "y = mcm(x) # MoreComplexModule.forward\n",
        "print('y: ', y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H0dekRQea1Ep",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**MoreComplexModule is a neural network!** It is a very shallow, simple neural network, but a neural net nonetheless. Now we need to train it to do something useful."
      ]
    },
    {
      "metadata": {
        "id": "ka3TZeGLFjmV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Loss Functions and Optimizers\n",
        "In order to train the parameters of our neural network (A and b in the case of MoreComplexModule), we'll need loss functions and optimizers. The loss function will define how far our neural networks are from the desired behavior by computing the loss, or error, that the network makes when given example inputs used for training. The optimizers will then change the parameters of the neural network with the goal of reducing the loss. Common loss functions can also be found in torch.nn."
      ]
    },
    {
      "metadata": {
        "id": "8FG1cmcm0EOa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### MSE Loss for Regression"
      ]
    },
    {
      "metadata": {
        "id": "8iaijwq6cJzM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Mean Squared Error computes the loss by taking the difference between \n",
        "# each entry in the tensor output by the neural network and the corresponding\n",
        "# entry in a tensor that contains the target values.\n",
        "# This is useful when you want your neural network to predict numerical values.\n",
        "\n",
        "mse_loss = nn.MSELoss()\n",
        "\n",
        "output = torch.eye(1, 1) # pretend neural net output\n",
        "target = torch.eye(1, 1) * 2 \n",
        "print(output, target)\n",
        "print(mse_loss(output, target))\n",
        "\n",
        "# The outputs of the neural network are often called predictions. \n",
        "\n",
        "predictions = torch.eye(2, 2)\n",
        "gold = torch.eye(2, 2) * 2\n",
        "print(predictions, gold)\n",
        "print(mse_loss(predictions, gold))\n",
        "\n",
        "# Target values are synonymous with ground truth or gold values. \n",
        "preds = torch.eye(3, 3)\n",
        "ground_truth = torch.eye(3, 3) * 2\n",
        "print(preds, ground_truth)\n",
        "print(mse_loss(preds, ground_truth))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NZonzxGS0Lay",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### NLLLoss for Classification"
      ]
    },
    {
      "metadata": {
        "id": "P93cKTZ2z_bG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# In classification problems, we assume that each possible input falls into \n",
        "# one of C classes, and we want the neural network to output the correct class.\n",
        "# In classification, predictions are somtimes called logits or scores\n",
        "# depending on the loss function and the exact output of the network. \n",
        "# Target values are often called labels.\n",
        "\n",
        "# NLLLoss, Negative Log Likelihood, is useful when your neural network outputs\n",
        "# probabilities for each of the classes.\n",
        "nll_loss = nn.NLLLoss()\n",
        "\n",
        "# some pretend classes\n",
        "classes = ['classA', 'classB']\n",
        "\n",
        "# pretend neural net output\n",
        "# each row is assumed to be associated with one example and\n",
        "# contains the probabilities for each class in some fixed order\n",
        "probabilites = torch.tensor([[.2, .8], [.1, .9], [.7, .3]]) \n",
        "labels = torch.tensor([classes.index('classA'), \n",
        "          classes.index('classB'), \n",
        "          classes.index('classA')])\n",
        "print(probabilites, labels)\n",
        "# note the .log(); NLLLoss assumes the inputs are log probabilities\n",
        "print(nll_loss(probabilites.log(), labels)) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B79R7Oxm0VsQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### CrossEntropyLoss for Classification"
      ]
    },
    {
      "metadata": {
        "id": "9U_TOHSL0ZEq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# CrossEntropyLoss is useful when your neural network outputs\n",
        "# scores for each of the classes, where the higher the score, the more likely \n",
        "# the class. This is by far more common than the case above.\n",
        "xe_loss = nn.CrossEntropyLoss()\n",
        "\n",
        "# pretend neural net output\n",
        "# each row is assumed to be associated with one example and\n",
        "# contains the scores for each class in some fixed order\n",
        "scores = torch.tensor([[2., 8], [1, 9], [7, 3]]) \n",
        "labels = torch.tensor([classes.index('classA'), \n",
        "          classes.index('classB'), \n",
        "          classes.index('classA')])\n",
        "print(scores, labels)\n",
        "# note the lack of .log(); CrossEntropyLoss calls LogSoftmax() on the scores,\n",
        "# which first takes the softmax to get logits and then applies a log function\n",
        "# Applying the softmax, then the log, and using NLLLoss is equivalent to using\n",
        "# scores with the CrossEntropyLoss.\n",
        "print(xe_loss(scores, labels)) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LtB3tpPCsIKi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Computing the loss won't change your network. You'll need to use an [optimizer](https://pytorch.org/docs/stable/optim.html#how-to-use-an-optimizer) to do that. The most common optimizers in NLP tend to be Stochastic Gradient Descent (SGD) and Adam. We won't dive into the technical details of either, but it is reasonable to try both on any given problem in NLP and choose which one seems to work best for your specific application.\n",
        "\n",
        "Training a neural network consits of iterating on the following:\n",
        "\n",
        "1.   Feed an input `x` forward through the network to get output `y`\n",
        "2.   Use `y` together with the target values to compute a loss\n",
        "3.   Call [backward](https://pytorch.org/docs/stable/autograd.html) on the loss to compute the direction of the next step\n",
        "4.    Use the optimizer to take the [step](https://pytorch.org/docs/stable/optim.html#optimizer-step) towards better parameters\n",
        "\n",
        "For now, all you need to know about 3) is that the backward pass assigns each parameter some responsibility for contributing to the loss. This credit assignment typically uses uses the backpropogation algorithm (backprop) to compute gradients, as in differential calculus. Fortunately, PyTorch implements autodifferentiation, so you (mostly) do not have to worry about how backprop actually works. You just have to remember to call `.backward()` on the loss to compute gradients.\n",
        "\n",
        "4) uses the credit assignment (gradients) from 3) to update the parameters based on how they contributed to the loss.\n"
      ]
    },
    {
      "metadata": {
        "id": "8b-W09Ld3wAo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Linear Regression"
      ]
    },
    {
      "metadata": {
        "id": "5Qhmd0SJshp5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "# Let's generate some random 2d data points\n",
        "inputs = torch.randn(20, 2)\n",
        "\n",
        "# And for this example, we'll try to predict the following function for input x:\n",
        "def target_generator(x):\n",
        "    return 2*x[0] + x[1]\n",
        "targets = torch.tensor([target_generator(i) for i in inputs]).unsqueeze(1)\n",
        "\n",
        "# Since this is just a linear function, \n",
        "# we can use a single linear layer\n",
        "model = nn.Linear(2, 1)\n",
        "\n",
        "# We are trying to predict real number values, so we use MSELoss\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "# When initializing an optimizer, \n",
        "# we have to give it access to the parameters of our model\n",
        "# When our optimizer takes a step to change our parameters based on the loss,\n",
        "# backward computes gradients that determine the direction of the step, and\n",
        "# the learning rate (lr) partially determines\n",
        "# how big the step should be in that direction.\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "n_epochs = 100 # how many times do we want to look at all of our examples\n",
        "losses = []\n",
        "for _ in range(n_epochs):\n",
        "    optimizer.zero_grad() \n",
        "    # the gradients should be zero'd out before each backward; \n",
        "    # otherwise they accumulate each time\n",
        "    preds = model(inputs)\n",
        "    loss = loss_fn(preds, targets)\n",
        "    losses.append(loss)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "print(f'Loss after {n_epochs} epochs: {losses[-1]}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6ncSS3JCqnm1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To verify that our network is learning, we can observe the the loss is decreasing over time."
      ]
    },
    {
      "metadata": {
        "id": "CZE7EVtFiEjC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "iterations = np.arange(len(losses))\n",
        "_ = plt.plot(iterations, losses, '', iterations, losses, '-')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3oboVIsD3X6p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As you can see, the loss drops to nearly zero, and it was nearly perfect on all examples after only a single epoch."
      ]
    },
    {
      "metadata": {
        "id": "1RiKlhNr-kZf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Data for Exercises 6-7\n",
        "Here we generate to sets of points and assign one set to be Class A and the other set Class B. In the following exercises, you will implement simple logistic regression and logistic regression with a neural network in order classify these points.\n"
      ]
    },
    {
      "metadata": {
        "id": "oulp7uzg-nkb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "# Generate 2 clusters of 100 2d vectors, each one distributed normally, using\n",
        "# only two calls of randn()\n",
        "classApoints = TODO\n",
        "classBpoints = TODO\n",
        "\n",
        "# Add the vector [1.0,3.0] to the first cluster and [3.0,1.0] to the second.\n",
        "classApoints += TODO\n",
        "classBpoints += TODO\n",
        "\n",
        "# Concatenate these two clusters along dimension 0 so that the points\n",
        "# distributed around [1.0, 3.0] all come first\n",
        "inputs = TODO\n",
        "\n",
        "# Create a tensor of target values, 0 for points for the first cluster and\n",
        "# 1 for the points in the second cluster. Make sure that these are LongTensors.\n",
        "classA = TODO\n",
        "classB = TODO\n",
        "targets = torch.cat([classA, classB])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Glu_2Bhv30E5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Exercise 6: Logistic Regression\n",
        "\n",
        "For this example, you will implement simple logistic regression for the data created above using a single linear layer.\n"
      ]
    },
    {
      "metadata": {
        "id": "OLfx7WAb3t5s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Set the random seed to 123 using manual_seed\n",
        "TODO\n",
        "\n",
        "# Initialize a Linear layer to output scores \n",
        "# for each class given the 2d examples\n",
        "model = TODO\n",
        "\n",
        "# Define your loss function\n",
        "loss_fn = TODO\n",
        "\n",
        "# Initialize an SGD optimizer with learning rate 0.1\n",
        "optimizer = TODO\n",
        "\n",
        "# Train the model for 100 epochs \n",
        "n_epochs = 100\n",
        "losses = []\n",
        "for _ in range(n_epochs):\n",
        "  optimizer.zero_grad() \n",
        "  preds = model(inputs)\n",
        "  loss = TODO\n",
        "  losses.append(loss)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "print(f'Anwswer to Exercise 6: Loss after {n_epochs} epochs: {losses[-1]}')\n",
        "      \n",
        "iterations = np.arange(len(losses))\n",
        "_ = plt.plot(iterations, losses, '', iterations, losses, '-')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8fh91p-0yjP_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Exercise 7: Logistic Regression with a Neural Network"
      ]
    },
    {
      "metadata": {
        "id": "RzeqqfoGG1qK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Define a module following these specifications:\n",
        "# __init__ takes self, in_size, hidden_size, and out_size.\n",
        "#          defines self.linear_to_hidden to be a linear layer with input\n",
        "#              size in_size and output size hidden_size\n",
        "#          defines self.hidden_to_output to be a second linear layer\n",
        "#              with input size hidden_size and output size out_size\n",
        "# forward takes self and x as input\n",
        "#         passes x through linear_to_hidden, then a tanh activation function,\n",
        "#         and then hidden_to_linear and returns the output\n",
        "#\n",
        "# This is is a neural network with 1 hidden layer \n",
        "# that contains hidden_size neurons.\n",
        "class NeuralNet(TODO):\n",
        "  def __init__(TODO):\n",
        "    super().__init__()\n",
        "    self.linear_to_hidden = TODO\n",
        "    self.hidden_to_output = TODO\n",
        "    \n",
        "  def forward(TODO):\n",
        "    return TODO\n",
        "\n",
        "# Set the random seed to 123 using manual_seed\n",
        "TODO\n",
        "\n",
        "# Initialize your new network to have in_size 2, hidden_size 6, and out_size 2\n",
        "# so that we can use the same data as in the previous exercise.\n",
        "model = TODO\n",
        "\n",
        "# Define your loss function\n",
        "loss_fn = TODO\n",
        "\n",
        "# Initialize an SGD optimizer with learning rate 0.3\n",
        "optimizer = TODO\n",
        "\n",
        "# Train the model for 100 epochs \n",
        "n_epochs = 100\n",
        "losses = []\n",
        "for _ in range(n_epochs):\n",
        "  optimizer.zero_grad() \n",
        "  preds = model(inputs)\n",
        "  loss = TODO\n",
        "  losses.append(loss)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "print(f'Answer to Exercise 7: Loss after {n_epochs} epochs: {losses[-1]}')\n",
        "\n",
        "iterations = np.arange(len(losses))\n",
        "_ = plt.plot(iterations, losses, '', iterations, losses, '-')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uyqdwOKOEhIL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## NLP\n"
      ]
    },
    {
      "metadata": {
        "id": "WBxbdS6mFnd3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "There are several standard processes and objects that come up early when working on an NLP problem."
      ]
    },
    {
      "metadata": {
        "id": "48qhsdLZoYbq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Tokenization"
      ]
    },
    {
      "metadata": {
        "id": "P2appezfFlkp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The first has to do with tokenization of text. The model needs to process sequences of characters as sequences of numbers, so one of the first steps in an NLP project usually deals with how those characters will be chunked up and grouped into tokens. These tokens are the smallest units of text that the model will understand. Tokenization can have a surprising impact on performance for some NLP models, so it is important to be familiar with the tradeoffs of any tokenizer. "
      ]
    },
    {
      "metadata": {
        "id": "SO2IU0HEnG6Q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's compare four different kinds of tokenization: character-level, which splits everything into individual characters, naive space-based, the [moses tokenization](https://www.nltk.org/_modules/nltk/tokenize/moses.html) common to machine translation, and [spaCy](https://spacy.io/api/tokenizer) tokenization. First, let's install the moses and spaCy tokenizers. This will take 5-10 minutes to load spaCy."
      ]
    },
    {
      "metadata": {
        "id": "B1vxToL5mkrM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip3 install nltk==3.2.5\n",
        "import nltk\n",
        "nltk.download('perluniprops')\n",
        "nltk.download('nonbreaking_prefixes')\n",
        "import nltk.tokenize.moses as moses\n",
        "\n",
        "## WARNING: THIS IS GOING TO TAKE A WHILE\n",
        "!pip3 install spacy\n",
        "import spacy\n",
        "spacy.cli.download(\"en\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fquz3FqrKPks",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Consider the following sentence\n",
        "sentence = \"I can't fathom the depths of the deepest neural nets.\"\n",
        "println('Example sentence: ', sentence)\n",
        "\n",
        "# We have plenty of options for how to tokenize such a sentence\n",
        "\n",
        "# The smaller the chunks, \n",
        "# the fewer possible tokens the model has to learn a meaning for. \n",
        "# Fewer tokens means more overlap between the kinds of tokens that appear \n",
        "# from sentence to sentence, word to word, or document to document. \n",
        "# At this extreme, we could handle everything at the character level.\n",
        "character_tokenization = [c for c in sentence]\n",
        "println('Character Tokenization: ', character_tokenization)\n",
        "\n",
        "# But words seem to have natural groupings of characters, \n",
        "# and we might want to give our model a natural sense of such word boundaries.\n",
        "word_tokenization = sentence.split()\n",
        "println('Space-based Word Tokenization: ', word_tokenization)\n",
        "\n",
        "# These shorter sequences would be faster to process, \n",
        "# and words like \"fathom\" can be learned as a unit\n",
        "# But words like \"can't\" and \"can\" will still be completely different tokens\n",
        "# Perhaps we could choose a strategy for separating the contraction out\n",
        "# so that the model can see it is composed of chunks of text that mean\n",
        "# 'can' and 'not'\n",
        "\n",
        "# Enter two common, free tokenizers, which make all sorts of decisions about \n",
        "# how tokenization should be done. Compare moses with spacy for example:\n",
        "\n",
        "moses_tokenizer = moses.MosesTokenizer()\n",
        "spacy_tokenizer = spacy.load('en')\n",
        "println('Moses Word Tokenization: ', moses_tokenizer.tokenize(sentence, escape=False))\n",
        "def spacy_tokenize(s):\n",
        "  return [t.text for t in spacy_tokenizer(s)]\n",
        "println('spaCy Word Tokenization: ', spacy_tokenize(sentence))\n",
        "\n",
        "# Another common tokenizer is CoreNLP, but it takes a little more work to set up\n",
        "# Regardless of which tokenizer you choose to use, try to get a sense of the \n",
        "# decisions you are letting it make for you"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bLDZGM3h2Dtd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Exercise 8\n",
        "Which of the four tokenizations demonstrated gives a tokenization that contains \"can\"?"
      ]
    },
    {
      "metadata": {
        "id": "3oMGdmW12RO9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Exercise 9\n",
        "Which of the four tokenizations demonstrated gives a tokenization that contains \"ca\"?"
      ]
    },
    {
      "metadata": {
        "id": "52Bx-avn2RL_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Exercise 10\n",
        "Which of the four tokenizations demonstrated gives a tokenization that contains \"nets.\"?"
      ]
    },
    {
      "metadata": {
        "id": "r1ne71wzQYvx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Vocabularies"
      ]
    },
    {
      "metadata": {
        "id": "ZjfxlNZwQMG7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Once you have your text tokenized, you'll likely need some kind of vocabulary object to keep track of the\n",
        "different kinds of tokens and the mapping between tokens and numerical values."
      ]
    },
    {
      "metadata": {
        "id": "izKwP34GEt7A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "document = \"One thing was certain, that the WHITE kitten had had nothing to do with it:--it was the black kitten’s fault entirely. For the white kitten had been having its face washed by the old cat for the last quarter of an hour (and bearing it pretty well, considering); so you see that it COULDN’T have had any hand in the mischief. The way Dinah washed her children’s faces was this: first she held the poor thing down by its ear with one paw, and then with the other paw she rubbed its face all over, the wrong way, beginning at the nose: and just now, as I said, she was hard at work on the white kitten, which was lying quite still and trying to purr--no doubt feeling that it was all meant for its good. But the black kitten had been finished with earlier in the afternoon, and so, while Alice was sitting curled up in a corner of the great arm-chair, half talking to herself and half asleep, the kitten had been having a grand game of romps with the ball of worsted Alice had been trying to wind up, and had been rolling it up and down till it had all come undone again; and there it was, spread over the hearth-rug, all knots and tangles, with the kitten running after its own tail in the middle. ‘Oh, you wicked little thing!’ cried Alice, catching up the kitten, and giving it a little kiss to make it understand that it was in disgrace. ‘Really, Dinah ought to have taught you better manners! You OUGHT, Dinah, you know you ought!’ she added, looking reproachfully at the old cat, and speaking in as cross a voice as she could manage--and then she scrambled back into the arm-chair, taking the kitten and the worsted with her, and began winding up the ball again. But she didn’t get on very fast, as she was talking all the time, sometimes to the kitten, and sometimes to herself. Kitty sat very demurely on her knee, pretending to watch the progress of the winding, and now and then putting out one paw and gently touching the ball, as if it would be glad to help, if it might. ‘Do you know what to-morrow is, Kitty?’ Alice began. ‘You’d have guessed if you’d been up in the window with me--only Dinah was making you tidy, so you couldn’t. I was watching the boys getting in sticks for the bonfire--and it wants plenty of sticks, Kitty! Only it got so cold, and it snowed so, they had to leave off. Never mind, Kitty, we’ll go and see the bonfire to-morrow.’ Here Alice wound two or three turns of the worsted round the kitten’s neck, just to see how it would look: this led to a scramble, in which the ball rolled down upon the floor, and yards and yards of it got unwound again. ‘Do you know, I was so angry, Kitty,’ Alice went on as soon as they were comfortably settled again, ‘when I saw all the mischief you had been doing, I was very nearly opening the window, and putting you out into the snow! And you’d have deserved it, you little mischievous darling! What have you got to say for yourself? Now don’t interrupt me!’ she went on, holding up one finger. ‘I’m going to tell you all your faults. Number one: you squeaked twice while Dinah was washing your face this morning. Now you can’t deny it, Kitty: I heard you! What’s that you say?’ (pretending that the kitten was speaking.) ‘Her paw went into your eye? Well, that’s YOUR fault, for keeping your eyes open--if you’d shut them tight up, it wouldn’t have happened. Now don’t make any more excuses, but listen! Number two: you pulled Snowdrop away by the tail just as I had put down the saucer of milk before her! What, you were thirsty, were you? How do you know she wasn’t thirsty too? Now for number three: you unwound every bit of the worsted while I wasn’t looking! ‘That’s three faults, Kitty, and you’ve not been punished for any of them yet. You know I’m saving up all your punishments for Wednesday week--Suppose they had saved up all MY punishments!’ she went on, talking more to herself than the kitten. ‘What WOULD they do at the end of a year? I should be sent to prison, I suppose, when the day came. Or--let me see--suppose each punishment was to be going without a dinner: then, when the miserable day came, I should have to go without fifty dinners at once! Well, I shouldn’t mind THAT much! I’d far rather go without them than eat them! ‘Do you hear the snow against the window-panes, Kitty? How nice and soft it sounds! Just as if some one was kissing the window all over outside. I wonder if the snow LOVES the trees and fields, that it kisses them so gently? And then it covers them up snug, you know, with a white quilt; and perhaps it says, “Go to sleep, darlings, till the summer comes again.” And when they wake up in the summer, Kitty, they dress themselves all in green, and dance about--whenever the wind blows--oh, that’s very pretty!’ cried Alice, dropping the ball of worsted to clap her hands. ‘And I do so WISH it was true! I’m sure the woods look sleepy in the autumn, when the leaves are getting brown. ‘Kitty, can you play chess? Now, don’t smile, my dear, I’m asking it seriously. Because, when we were playing just now, you watched just as if you understood it: and when I said “Check!” you purred! Well, it WAS a nice check, Kitty, and really I might have won, if it hadn’t been for that nasty Knight, that came wiggling down among my pieces. Kitty, dear, let’s pretend--’ And here I wish I could tell you half the things Alice used to say, beginning with her favourite phrase ‘Let’s pretend.’ She had had quite a long argument with her sister only the day before--all because Alice had begun with ‘Let’s pretend we’re kings and queens;’ and her sister, who liked being very exact, had argued that they couldn’t, because there were only two of them, and Alice had been reduced at last to say, ‘Well, YOU can be one of them then, and I’LL be all the rest.’ And once she had really frightened her old nurse by shouting suddenly in her ear, ‘Nurse! Do let’s pretend that I’m a hungry hyaena, and you’re a bone.’ But this is taking us away from Alice’s speech to the kitten. ‘Let’s pretend that you’re the Red Queen, Kitty! Do you know, I think if you sat up and folded your arms, you’d look exactly like her. Now do try, there’s a dear!’ And Alice got the Red Queen off the table, and set it up before the kitten as a model for it to imitate: however, the thing didn’t succeed, principally, Alice said, because the kitten wouldn’t fold its arms properly. So, to punish it, she held it up to the Looking-glass, that it might see how sulky it was--‘and if you’re not good directly,’ she added, ‘I’ll put you through into Looking-glass House. How would you like THAT?’ ‘Now, if you’ll only attend, Kitty, and not talk so much, I’ll tell you all my ideas about Looking-glass House. First, there’s the room you can see through the glass--that’s just the same as our drawing room, only the things go the other way. I can see all of it when I get upon a chair--all but the bit behind the fireplace. Oh! I do so wish I could see THAT bit! I want so much to know whether they’ve a fire in the winter: you never CAN tell, you know, unless our fire smokes, and then smoke comes up in that room too--but that may be only pretence, just to make it look as if they had a fire. Well then, the books are something like our books, only the words go the wrong way; I know that, because I’ve held up one of our books to the glass, and then they hold up one in the other room. ‘How would you like to live in Looking-glass House, Kitty? I wonder if they’d give you milk in there? Perhaps Looking-glass milk isn’t good to drink--But oh, Kitty! now we come to the passage. You can just see a little PEEP of the passage in Looking-glass House, if you leave the door of our drawing-room wide open: and it’s very like our passage as far as you can see, only you know it may be quite different on beyond. Oh, Kitty! how nice it would be if we could only get through into Looking-glass House! I’m sure it’s got, oh! such beautiful things in it! Let’s pretend there’s a way of getting through into it, somehow, Kitty. Let’s pretend the glass has got all soft like gauze, so that we can get through. Why, it’s turning into a sort of mist now, I declare! It’ll be easy enough to get through--’ She was up on the chimney-piece while she said this, though she hardly knew how she had got there. And certainly the glass WAS beginning to melt away, just like a bright silvery mist. In another moment Alice was through the glass, and had jumped lightly down into the Looking-glass room. The very first thing she did was to look whether there was a fire in the fireplace, and she was quite pleased to find that there was a real one, blazing away as brightly as the one she had left behind. ‘So I shall be as warm here as I was in the old room,’ thought Alice: ‘warmer, in fact, because there’ll be no one here to scold me away from the fire. Oh, what fun it’ll be, when they see me through the glass in here, and can’t get at me!’ Then she began looking about, and noticed that what could be seen from the old room was quite common and uninteresting, but that all the rest was as different as possible. For instance, the pictures on the wall next the fire seemed to be all alive, and the very clock on the chimney-piece (you know you can only see the back of it in the Looking-glass) had got the face of a little old man, and grinned at her. ‘They don’t keep this room so tidy as the other,’ Alice thought to herself, as she noticed several of the chessmen down in the hearth among the cinders: but in another moment, with a little ‘Oh!’ of surprise, she was down on her hands and knees watching them. The chessmen were walking about, two and two! ‘Here are the Red King and the Red Queen,’ Alice said (in a whisper, for fear of frightening them), ‘and there are the White King and the White Queen sitting on the edge of the shovel--and here are two castles walking arm in arm--I don’t think they can hear me,’ she went on, as she put her head closer down, ‘and I’m nearly sure they can’t see me. I feel somehow as if I were invisible--’ Here something began squeaking on the table behind Alice, and made her turn her head just in time to see one of the White Pawns roll over and begin kicking: she watched it with great curiosity to see what would happen next. ‘It is the voice of my child!’ the White Queen cried out as she rushed past the King, so violently that she knocked him over among the cinders. ‘My precious Lily! My imperial kitten!’ and she began scrambling wildly up the side of the fender. ‘Imperial fiddlestick!’ said the King, rubbing his nose, which had been hurt by the fall. He had a right to be a LITTLE annoyed with the Queen, for he was covered with ashes from head to foot. Alice was very anxious to be of use, and, as the poor little Lily was nearly screaming herself into a fit, she hastily picked up the Queen and set her on the table by the side of her noisy little daughter. The Queen gasped, and sat down: the rapid journey through the air had quite taken away her breath and for a minute or two she could do nothing but hug the little Lily in silence. As soon as she had recovered her breath a little, she called out to the White King, who was sitting sulkily among the ashes, ‘Mind the volcano!’ ‘What volcano?’ said the King, looking up anxiously into the fire, as if he thought that was the most likely place to find one. ‘Blew--me--up,’ panted the Queen, who was still a little out of breath. ‘Mind you come up--the regular way--don’t get blown up!’ Alice watched the White King as he slowly struggled up from bar to bar, till at last she said, ‘Why, you’ll be hours and hours getting to the table, at that rate. I’d far better help you, hadn’t I?’ But the King took no notice of the question: it was quite clear that he could neither hear her nor see her. So Alice picked him up very gently, and lifted him across more slowly than she had lifted the Queen, that she mightn’t take his breath away: but, before she put him on the table, she thought she might as well dust him a little, he was so covered with ashes. She said afterwards that she had never seen in all her life such a face as the King made, when he found himself held in the air by an invisible hand, and being dusted: he was far too much astonished to cry out, but his eyes and his mouth went on getting larger and larger, and rounder and rounder, till her hand shook so with laughing that she nearly let him drop upon the floor. ‘Oh! PLEASE don’t make such faces, my dear!’ she cried out, quite forgetting that the King couldn’t hear her. ‘You make me laugh so that I can hardly hold you! And don’t keep your mouth so wide open! All the ashes will get into it--there, now I think you’re tidy enough!’ she added, as she smoothed his hair, and set him upon the table near the Queen. The King immediately fell flat on his back, and lay perfectly still: and Alice was a little alarmed at what she had done, and went round the room to see if she could find any water to throw over him. However, she could find nothing but a bottle of ink, and when she got back with it she found he had recovered, and he and the Queen were talking together in a frightened whisper--so low, that Alice could hardly hear what they said. The King was saying, ‘I assure, you my dear, I turned cold to the very ends of my whiskers!’ To which the Queen replied, ‘You haven’t got any whiskers.’ ‘The horror of that moment,’ the King went on, ‘I shall never, NEVER forget!’ ‘You will, though,’ the Queen said, ‘if you don’t make a memorandum of it.’ Alice looked on with great interest as the King took an enormous memorandum-book out of his pocket, and began writing. A sudden thought struck her, and she took hold of the end of the pencil, which came some way over his shoulder, and began writing for him. The poor King looked puzzled and unhappy, and struggled with the pencil for some time without saying anything; but Alice was too strong for him, and at last he panted out, ‘My dear! I really MUST get a thinner pencil. I can’t manage this one a bit; it writes all manner of things that I don’t intend--’ ‘What manner of things?’ said the Queen, looking over the book (in which Alice had put ‘THE WHITE KNIGHT IS SLIDING DOWN THE POKER. HE BALANCES VERY BADLY’) ‘That’s not a memorandum of YOUR feelings!’ There was a book lying near Alice on the table, and while she sat watching the White King (for she was still a little anxious about him, and had the ink all ready to throw over him, in case he fainted again), she turned over the leaves, to find some part that she could read, ‘--for it’s all in some language I don’t know,’ she said to herself. It was like this.\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nwhFbLtTQXLy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# The simplest vocabulary will likely need to keep track of all the tokens,\n",
        "# but it is generally a good idea to keep track of how frequent each token is\n",
        "# and then keep the vocabulary ordered. Here we order first by word, then by \n",
        "# frequency, but any ordering that guranatees that anyone creating the \n",
        "# vocabulary from the same dataset would get exactly \n",
        "# the same order will work\n",
        "tokens_with_counts = collections.Counter(spacy_tokenize(document))\n",
        "tokens_with_counts = sorted(tokens_with_counts.items(), key=lambda tup: tup[0])\n",
        "tokens_with_counts.sort(key=lambda tup: tup[1], reverse=True)\n",
        "print(tokens_with_counts)\n",
        "\n",
        "# If our vocabulary is too big, we might want to prune off some of the rare\n",
        "# words, often those that occur fewer than 4 times, but we have only the one\n",
        "# document for now, so let's keep word with more than 1 occurrence\n",
        "print(len(tokens_with_counts))\n",
        "tokens_with_counts_pruned = [t for t in tokens_with_counts if t[1] > 1]\n",
        "print(len(tokens_with_counts_pruned))\n",
        "\n",
        "# Now that we have our words in order, we can let this define the mapping \n",
        "# we will need between words and indices:\n",
        "index_to_token = [t[0] for t in tokens_with_counts_pruned]\n",
        "token_to_index = {t: i for i, t in enumerate(index_to_token)}\n",
        "\n",
        "# With this we can convert our tokenized document to a sequence of numbers.\n",
        "# This process of numericalization will be an early step in most NLP pipelines."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h_kIhfB9KovN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Congratulations, you made it through the lab! You have become acquainted with the basics of PyTorch and some of the tools you will need for NLP applications. Don't forget to head back to Trailhead to enter your answers to the exercises in the quiz and get points. "
      ]
    }
  ]
}